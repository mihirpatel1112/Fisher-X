{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "809182c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c715e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../models/data/combined_openaq_weather.csv')\n",
    "air_quality_params = ['co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm25', 'so2']\n",
    "weather_features = ['temp', 'dwpt', 'rhum', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun', 'coco']\n",
    "time_columns = ['time']\n",
    "df.drop(columns=['datetimeLocal'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "732d6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dataframes = {}\n",
    "for param in air_quality_params:\n",
    "    selected_columns = time_columns + [param] + weather_features\n",
    "    param_df = df[selected_columns].copy()\n",
    "    param_df = param_df.dropna(subset=[param])\n",
    "    parameter_dataframes[param] = param_df\\\n",
    "\n",
    "\n",
    "dfs=parameter_dataframes\n",
    "\n",
    "params = ['co', 'no', 'no2', 'nox', 'o3', 'pm10', 'pm25', 'so2']\n",
    "weather_cols = ['temp', 'dwpt', 'rhum', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun', 'coco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7483b73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CO | orig=11  keep= 9  feats=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "    NO | orig=11  keep= 8  feats=['temp', 'dwpt', 'rhum', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "   NO2 | orig=11  keep= 9  feats=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "   NOX | orig=11  keep= 9  feats=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "    O3 | orig=11  keep= 9  feats=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "  PM10 | orig=11  keep= 9  feats=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "  PM25 | orig=11  keep= 9  feats=['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd', 'wpgt', 'pres', 'coco']\n",
      "   SO2 | orig=11  keep= 2  feats=['temp', 'dwpt']\n"
     ]
    }
   ],
   "source": [
    "#Feature selection using XGBoost\n",
    "def xgb_feature_select(dfs, params, weather_cols, thresh=0.01,\n",
    "                       n_estimators=100, random_state=42, verbose=True):\n",
    "    selected = {}\n",
    "    for p in params:\n",
    "        df = dfs[p]\n",
    "        model = xgb.XGBRegressor(n_estimators=n_estimators, random_state=random_state, verbosity=0)\n",
    "        model.fit(df[weather_cols], df[p])\n",
    "        imps = model.feature_importances_\n",
    "        keep = [f for f, w in zip(weather_cols, imps) if w > thresh] or [weather_cols[int(imps.argmax())]]\n",
    "        selected[p] = df[['time', p] + keep]\n",
    "        if verbose:\n",
    "            print(f\"{p.upper():>6} | orig={len(weather_cols):2d}  keep={len(keep):2d}  feats={keep}\")\n",
    "    return selected\n",
    "\n",
    "# Usage\n",
    "selected_dfs = xgb_feature_select(dfs, params, weather_cols, thresh=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "972c4ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO: dropped 113 rows\n",
      "NO: dropped 63 rows\n",
      "NO2: dropped 113 rows\n",
      "NOX: dropped 87 rows\n",
      "O3: dropped 12 rows\n",
      "PM10: dropped 35 rows\n",
      "PM25: dropped 39 rows\n",
      "SO2: dropped 33 rows\n",
      "TOTAL dropped: 495\n"
     ]
    }
   ],
   "source": [
    "#Removing outliers using z score method\n",
    "total_dropped = 0\n",
    "for p in params:\n",
    "    df = selected_dfs[p]\n",
    "    x = df[p].astype(float)\n",
    "    m, sd = x.mean(), x.std(ddof=0)\n",
    "\n",
    "    if sd == 0 or not np.isfinite(sd):\n",
    "        dropped = 0\n",
    "        keep = np.ones(len(df), dtype=bool)\n",
    "    else:\n",
    "        z = (x - m) / sd\n",
    "        keep = np.abs(z) <= 3\n",
    "        dropped = int((~keep).sum())\n",
    "\n",
    "    selected_dfs[p] = df[keep].reset_index(drop=True)\n",
    "    total_dropped += dropped\n",
    "    print(f\"{p.upper()}: dropped {dropped} rows\")\n",
    "\n",
    "print(f\"TOTAL dropped: {total_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcf13a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO: dropped 2 â†’ pres(156.6), wpgt(39.9)\n",
      "NO: dropped 2 â†’ pres(156.4), wpgt(40.0)\n",
      "NO2: dropped 2 â†’ pres(156.7), wpgt(39.9)\n",
      "NOX: dropped 2 â†’ pres(156.8), wpgt(39.9)\n",
      "O3: dropped 2 â†’ pres(157.6), wpgt(40.3)\n",
      "PM10: dropped 2 â†’ pres(162.5), wpgt(40.9)\n",
      "PM25: dropped 2 â†’ pres(157.3), wpgt(40.2)\n",
      "SO2: dropped 0 â†’ none\n"
     ]
    }
   ],
   "source": [
    "#Removing features with high multicollinearity using VIF\n",
    "def drop_high_vif(cleaned_dfs, params, thresh=10.0):\n",
    "    for p in params:\n",
    "        df = cleaned_dfs[p]\n",
    "        feats = (df.drop(columns=[c for c in ['time', p] if c in df.columns])\n",
    "                   .select_dtypes(include=[np.number]))\n",
    "        feats = feats.loc[:, feats.std(ddof=0) > 0]\n",
    "        dropped = []\n",
    "        while feats.shape[1] > 1:\n",
    "            X = feats.replace([np.inf, -np.inf], np.nan).fillna(feats.median(numeric_only=True))\n",
    "            vifs = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)\n",
    "            bad = vifs.idxmax()\n",
    "            if vifs[bad] <= thresh: break\n",
    "            feats = feats.drop(columns=[bad]); dropped.append(f\"{bad}({vifs[bad]:.1f})\")\n",
    "        cleaned_dfs[p] = df.drop(columns=[c.split('(')[0] for c in dropped], errors='ignore')\n",
    "        print(f\"{p.upper()}: dropped {len(dropped)} â†’ {', '.join(dropped) if dropped else 'none'}\")\n",
    "\n",
    "# Usage\n",
    "drop_high_vif(selected_dfs, params, thresh=10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecdb372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param      skew  log_applied  shift_used  skew_after  improve\n",
      " PM10  3.502768         True         0.0    0.210075 3.292693\n",
      "   NO  3.241736         True         1.0    3.229749 0.011987\n",
      "  NOX  2.100233         True         1.0    2.083399 0.016834\n",
      " PM25  2.080380         True         0.9    0.509982 1.570398\n",
      "  NO2  1.667036         True         1.0    1.656979 0.010057\n",
      "   O3 -0.503169        False         0.0         NaN      NaN\n",
      "  SO2  0.347757        False         0.0         NaN      NaN\n",
      "   CO -0.033095        False         0.0         NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "def skew_check_and_log_transform(final_dfs, params, thresh=1.0):\n",
    "    transformed_dfs, rows = {}, []\n",
    "    for p in params:\n",
    "        df = final_dfs[p].copy()\n",
    "        x = df[p].astype(float)\n",
    "        sk0 = float(skew(x, nan_policy='omit'))\n",
    "        do_log = abs(sk0) >= thresh\n",
    "\n",
    "        if do_log:\n",
    "            shift = max(0.0, 1.0 - x.min())  # ensure strictly >0 before log\n",
    "            x_pos = x + shift\n",
    "            df[f'{p}_log'] = np.log(x_pos)\n",
    "            sk1 = float(skew(df[f'{p}_log'], nan_policy='omit'))\n",
    "        else:\n",
    "            shift, sk1 = 0.0, np.nan\n",
    "\n",
    "        transformed_dfs[p] = df\n",
    "        rows.append({\n",
    "            \"param\": p.upper(),\n",
    "            \"skew\": sk0,\n",
    "            \"log_applied\": do_log,\n",
    "            \"shift_used\": shift if do_log else 0.0,\n",
    "            \"skew_after\": sk1 if do_log else None,\n",
    "            \"improve\": (abs(sk0) - abs(sk1)) if do_log else None\n",
    "        })\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values(\"skew\", key=lambda s: s.abs(), ascending=False)\n",
    "    print(summary.to_string(index=False))\n",
    "    return transformed_dfs, summary\n",
    "\n",
    "# Usage:\n",
    "transformed_dfs, skew_summary = skew_check_and_log_transform(selected_dfs, params, thresh=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6835388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO: scaled 7 features â†’ (4776, 9)\n",
      "NO: scaled 7 features â†’ (4826, 9)\n",
      "NO2: scaled 8 features â†’ (4776, 10)\n",
      "NOX: scaled 8 features â†’ (4802, 10)\n",
      "O3: scaled 7 features â†’ (4877, 9)\n",
      "PM10: scaled 8 features â†’ (4854, 10)\n",
      "PM25: scaled 8 features â†’ (4850, 10)\n",
      "SO2: scaled 2 features â†’ (4856, 4)\n",
      "\n",
      "Summary:\n",
      "CO: 7 feats, target=co\n",
      "NO: 7 feats, target=no_log\n",
      "NO2: 8 feats, target=no2_log\n",
      "NOX: 8 feats, target=nox_log\n",
      "O3: 7 feats, target=o3\n",
      "PM10: 8 feats, target=pm10_log\n",
      "PM25: 8 feats, target=pm25_log\n",
      "SO2: 2 feats, target=so2\n"
     ]
    }
   ],
   "source": [
    "def minmax_scale_all(transformed_dfs, params):\n",
    "    scaled_dfs, scalers, summary = {}, {}, []\n",
    "    for p in params:\n",
    "        df = transformed_dfs[p]\n",
    "        target = f\"{p}_log\" if f\"{p}_log\" in df.columns else p\n",
    "        feats = [c for c in df.columns if c not in ('time', p) and not c.endswith('_shifted')]\n",
    "        sc = MinMaxScaler().fit(df[feats]) if feats else None\n",
    "        scaled = (pd.DataFrame(sc.transform(df[feats]), columns=[f\"{c}_scaled\" for c in feats], index=df.index)\n",
    "                  if feats else pd.DataFrame(index=df.index))\n",
    "        scaled_df = pd.concat([df[['time', target]], scaled], axis=1)\n",
    "        scaled_dfs[p], scalers[p] = scaled_df, {\"scaler\": sc, \"feature_names\": feats}\n",
    "        print(f\"{p.upper()}: scaled {len(feats)} features â†’ {scaled_df.shape}\")\n",
    "        summary.append((p.upper(), target, len(feats)))\n",
    "    print(\"\\nSummary:\", *[f\"{p}: {n} feats, target={t}\" for p,t,n in summary], sep=\"\\n\")\n",
    "    return scaled_dfs, scalers\n",
    "\n",
    "# Usage:\n",
    "scaled_dfs, scalers = minmax_scale_all(transformed_dfs, params)\n",
    "\n",
    "#Apply sclaer at inference TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8622dd6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['temp', 'dwpt', 'rhum', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun', 'coco'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(pack[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m].predict(x)[\u001b[32m0\u001b[39m])\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m results = \u001b[43mtrain_simple_xgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweather_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m yhat_pm25_now = predict_current(scaled_dfs, \u001b[33m'\u001b[39m\u001b[33mpm25\u001b[39m\u001b[33m'\u001b[39m, weather_cols, results[\u001b[33m'\u001b[39m\u001b[33mpm25\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_simple_xgb\u001b[39m\u001b[34m(dfs, params, weather_cols, test_frac, n_estimators, random_state)\u001b[39m\n\u001b[32m     11\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_lag1\u001b[39m\u001b[33m'\u001b[39m] = df[p].shift(\u001b[32m1\u001b[39m)\n\u001b[32m     13\u001b[39m feats = weather_cols + [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_lag1\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m tmp = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.dropna()   \u001b[38;5;66;03m# drop rows where lag1 or weather is missing\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tmp) < \u001b[32m100\u001b[39m:\n\u001b[32m     16\u001b[39m     results[p] = {\u001b[33m\"\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnot enough data\u001b[39m\u001b[33m\"\u001b[39m}; \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['temp', 'dwpt', 'rhum', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun', 'coco'] not in index\""
     ]
    }
   ],
   "source": [
    "def train_simple_xgb(dfs, params, weather_cols, test_frac=0.2, n_estimators=100, random_state=42):\n",
    "    \"\"\"\n",
    "    dfs[param] must have columns: ['time', param] + weather_cols (hourly or at least sorted by time).\n",
    "    Features = weather_cols + f'{param}_lag1'  (lag is t-1h). Target = param at time t.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for p in params:\n",
    "        df = dfs[p].copy()\n",
    "        if 'time' in df.columns:\n",
    "            df = df.sort_values('time').set_index('time')\n",
    "        df[f'{p}_lag1'] = df[p].shift(1)\n",
    "\n",
    "        feats = weather_cols + [f'{p}_lag1']\n",
    "        tmp = df[feats + [p]].dropna()   # drop rows where lag1 or weather is missing\n",
    "        if len(tmp) < 100:\n",
    "            results[p] = {\"note\": \"not enough data\"}; continue\n",
    "\n",
    "        split = int(len(tmp) * (1 - test_frac))\n",
    "        Xtr, Xte = tmp.iloc[:split][feats], tmp.iloc[split:][feats]\n",
    "        ytr, yte = tmp.iloc[:split][p],     tmp.iloc[split:][p]\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=n_estimators, learning_rate=0.05, max_depth=6,\n",
    "            subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "            random_state=random_state, verbosity=0\n",
    "        )\n",
    "        model.fit(Xtr, ytr, eval_set=[(Xte, yte)], verbose=False)\n",
    "\n",
    "        yhat = pd.Series(model.predict(Xte), index=yte.index)\n",
    "        results[p] = {\n",
    "            \"model\": model,\n",
    "            \"features\": feats,\n",
    "            \"mae\": float(mean_absolute_error(yte, yhat)),\n",
    "            \"r2\":  float(r2_score(yte, yhat))\n",
    "        }\n",
    "        print(f\"{p.upper()}: MAE={results[p]['mae']:.3f} | R2={results[p]['r2']:.3f}\")\n",
    "    return results\n",
    "\n",
    "def predict_current(dfs, param, weather_cols, pack):\n",
    "    \"\"\"Predict param at the latest time t using weather@t + lag1.\"\"\"\n",
    "    df = dfs[param].copy()\n",
    "    if 'time' in df.columns:\n",
    "        df = df.sort_values('time').set_index('time')\n",
    "    df[f'{param}_lag1'] = df[param].shift(1)\n",
    "    feats = pack[\"features\"]\n",
    "    x = df.iloc[[-1]][feats].dropna(axis=1, how='all')  # use latest row\n",
    "    return float(pack[\"model\"].predict(x)[0])\n",
    "\n",
    "# Usage:\n",
    "results = train_simple_xgb(scaled_dfs, params, weather_cols)\n",
    "yhat_pm25_now = predict_current(scaled_dfs, 'pm25', weather_cols, results['pm25'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6792568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple XGBoost with 1-hour lag features\n",
    "def create_lag_features_simple(scaled_dfs, params):\n",
    "    \"\"\"Create 1-hour lag features for weather and target parameter\"\"\"\n",
    "    lag_dfs = {}\n",
    "    \n",
    "    for param in params:\n",
    "        df = scaled_dfs[param].copy()\n",
    "        \n",
    "        # Convert time to datetime and sort\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df = df.sort_values('time').reset_index(drop=True)\n",
    "        \n",
    "        # Get target column (log-transformed if available)\n",
    "        target_col = f'{param}_log' if f'{param}_log' in df.columns else param\n",
    "        \n",
    "        # Create 1-hour lag for target parameter\n",
    "        df[f'{target_col}_lag1'] = df[target_col].shift(1)\n",
    "        \n",
    "        # Create 1-hour lag for all scaled weather features\n",
    "        weather_features = [col for col in df.columns if col.endswith('_scaled')]\n",
    "        for feature in weather_features:\n",
    "            df[f'{feature}_lag1'] = df[feature].shift(1)\n",
    "        \n",
    "        # Add basic time features\n",
    "        df['hour'] = df['time'].dt.hour\n",
    "        df['day_of_week'] = df['time'].dt.dayofweek\n",
    "        df['month'] = df['time'].dt.month\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        \n",
    "        # Store the enhanced dataframe\n",
    "        lag_dfs[param] = df\n",
    "        \n",
    "        print(f\"{param.upper()}: Added lag features â†’ {df.shape}\")\n",
    "    \n",
    "    return lag_dfs\n",
    "\n",
    "# Create lag features\n",
    "lag_dfs = create_lag_features_simple(scaled_dfs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Simple XGBoost Models with 1-hour lag\n",
    "def train_simple_lag_models(lag_dfs, params):\n",
    "    \"\"\"Train XGBoost models using current weather + 1-hour lag features\"\"\"\n",
    "    simple_models = {}\n",
    "    \n",
    "    for param in params:\n",
    "        print(f\"\\nðŸ”„ Training {param.upper()} model...\")\n",
    "        \n",
    "        df = lag_dfs[param].copy()\n",
    "        \n",
    "        # Get target column\n",
    "        target_col = f'{param}_log' if f'{param}_log' in df.columns else param\n",
    "        \n",
    "        # Select features: current weather + lag features + time features\n",
    "        feature_cols = []\n",
    "        \n",
    "        # Current scaled weather features\n",
    "        current_weather = [col for col in df.columns if col.endswith('_scaled') and '_lag1' not in col]\n",
    "        feature_cols.extend(current_weather)\n",
    "        \n",
    "        # 1-hour lag weather features\n",
    "        lag_weather = [col for col in df.columns if col.endswith('_scaled_lag1')]\n",
    "        feature_cols.extend(lag_weather)\n",
    "        \n",
    "        # 1-hour lag target parameter\n",
    "        target_lag = f'{target_col}_lag1'\n",
    "        if target_lag in df.columns:\n",
    "            feature_cols.append(target_lag)\n",
    "        \n",
    "        # Time features\n",
    "        time_features = ['hour', 'day_of_week', 'month', 'is_weekend']\n",
    "        feature_cols.extend(time_features)\n",
    "        \n",
    "        # Remove rows with NaN (due to lag features)\n",
    "        df_clean = df.dropna(subset=feature_cols + [target_col])\n",
    "        \n",
    "        if len(df_clean) < 100:\n",
    "            print(f\"   âš ï¸ Not enough data for {param.upper()}: {len(df_clean)} rows\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df_clean[feature_cols]\n",
    "        y = df_clean[target_col]\n",
    "        \n",
    "        # Time-based split (80% train, 20% test)\n",
    "        split_idx = int(0.8 * len(df_clean))\n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "        \n",
    "        # Train XGBoost model\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Store results\n",
    "        simple_models[param] = {\n",
    "            'model': model,\n",
    "            'target_col': target_col,\n",
    "            'feature_cols': feature_cols,\n",
    "            'test_r2': test_r2,\n",
    "            'test_mae': test_mae,\n",
    "            'is_log_transformed': '_log' in target_col,\n",
    "            'train_samples': len(X_train),\n",
    "            'test_samples': len(X_test)\n",
    "        }\n",
    "        \n",
    "        print(f\"   Features: {len(feature_cols)}\")\n",
    "        print(f\"   Train samples: {len(X_train)}\")\n",
    "        print(f\"   Test RÂ²: {test_r2:.4f}\")\n",
    "        print(f\"   Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"   âœ… Completed\")\n",
    "    \n",
    "    return simple_models\n",
    "\n",
    "# Train the models\n",
    "simple_models = train_simple_lag_models(lag_dfs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary and Prediction Function\n",
    "print(\"\\nðŸ“Š SIMPLE LAG MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Parameter':<8} {'RÂ²':<8} {'MAE':<8} {'Features':<8} {'Log Trans':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for param, results in simple_models.items():\n",
    "    log_status = \"Yes\" if results['is_log_transformed'] else \"No\"\n",
    "    print(f\"{param.upper():<8} {results['test_r2']:<8.4f} {results['test_mae']:<8.4f} \"\n",
    "          f\"{len(results['feature_cols']):<8} {log_status:<10}\")\n",
    "\n",
    "def predict_next_hour(param, current_weather, lag_weather=None, lag_param=None, \n",
    "                     current_time=None, models=simple_models, scalers=scalers):\n",
    "    \"\"\"\n",
    "    Predict next hour's pollutant concentration\n",
    "    \n",
    "    Args:\n",
    "        param: Parameter to predict ('co', 'no2', etc.)\n",
    "        current_weather: Dict of current weather {feature: value}\n",
    "        lag_weather: Dict of 1-hour ago weather (optional, uses current if None)\n",
    "        lag_param: Previous hour's parameter value (optional, uses 0 if None)\n",
    "        current_time: Current datetime (optional, uses now if None)\n",
    "    \"\"\"\n",
    "    if param not in models:\n",
    "        return f\"No model available for {param}\"\n",
    "    \n",
    "    if current_time is None:\n",
    "        current_time = pd.Timestamp.now()\n",
    "    \n",
    "    if lag_weather is None:\n",
    "        lag_weather = current_weather.copy()\n",
    "    \n",
    "    if lag_param is None:\n",
    "        lag_param = 0.0\n",
    "    \n",
    "    model_info = models[param]\n",
    "    model = model_info['model']\n",
    "    scaler_info = scalers[param]\n",
    "    \n",
    "    # Scale current weather features\n",
    "    weather_features = scaler_info['feature_names']\n",
    "    current_values = [current_weather.get(f, 0.0) for f in weather_features]\n",
    "    lag_values = [lag_weather.get(f, 0.0) for f in weather_features]\n",
    "    \n",
    "    if scaler_info['scaler']:\n",
    "        current_scaled = scaler_info['scaler'].transform([current_values])[0]\n",
    "        lag_scaled = scaler_info['scaler'].transform([lag_values])[0]\n",
    "    else:\n",
    "        current_scaled = current_values\n",
    "        lag_scaled = lag_values\n",
    "    \n",
    "    # Prepare feature vector\n",
    "    feature_vector = []\n",
    "    \n",
    "    # Current weather (scaled)\n",
    "    for i, feature in enumerate(weather_features):\n",
    "        feature_vector.append(current_scaled[i])\n",
    "    \n",
    "    # Lag weather (scaled)\n",
    "    for i, feature in enumerate(weather_features):\n",
    "        feature_vector.append(lag_scaled[i])\n",
    "    \n",
    "    # Lag parameter value (log-transformed if needed)\n",
    "    if model_info['is_log_transformed'] and lag_param > 0:\n",
    "        lag_param_processed = np.log(lag_param)\n",
    "    else:\n",
    "        lag_param_processed = lag_param\n",
    "    feature_vector.append(lag_param_processed)\n",
    "    \n",
    "    # Time features\n",
    "    feature_vector.extend([\n",
    "        current_time.hour,\n",
    "        current_time.dayofweek,\n",
    "        current_time.month,\n",
    "        1 if current_time.dayofweek >= 5 else 0  # is_weekend\n",
    "    ])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict([feature_vector])[0]\n",
    "    \n",
    "    # Back-transform if log-transformed\n",
    "    if model_info['is_log_transformed']:\n",
    "        prediction = np.exp(prediction)\n",
    "    \n",
    "    return max(0, prediction)  # Ensure non-negative\n",
    "\n",
    "# Example prediction\n",
    "example_weather = {\n",
    "    'temp': 25.0, 'dwpt': 18.0, 'rhum': 65.0, 'prcp': 0.0, 'snow': 0.0,\n",
    "    'wdir': 180.0, 'wspd': 10.0, 'wpgt': 15.0, 'pres': 1013.25, 'tsun': 8.0, 'coco': 3.0\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ”® EXAMPLE PREDICTIONS (next hour):\")\n",
    "print(\"Weather conditions: Temp=25Â°C, Wind=10km/h, Humidity=65%\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for param in ['co', 'no2', 'pm25', 'o3']:  # Example subset\n",
    "    if param in simple_models:\n",
    "        pred = predict_next_hour(param, example_weather, lag_param=20.0)\n",
    "        print(f\"{param.upper():<6}: {pred:.4f} Î¼g/mÂ³\")\n",
    "\n",
    "print(f\"\\nâœ… Simple lag models ready for prediction!\")\n",
    "print(f\"ðŸ“ Use predict_next_hour(param, weather_dict) to make predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
